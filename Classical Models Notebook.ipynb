{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "7a95e52626a44877b8ae0bac2b5a6e94",
    "deepnote_cell_type": "code",
    "execution_context_id": "83d24c1b-8837-4559-b2e3-29a45d51476a",
    "execution_millis": 1154,
    "execution_start": 1735863554818,
    "source_hash": "8ca6a091"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikeras) (3.7.0)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikeras) (1.6.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.12.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\marya\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->scikeras) (23.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\marya\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->scikeras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\marya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_get_column_indices' from 'sklearn.utils' (c:\\Users\\marya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Set random seed for reproducibility\u001b[39;00m\n\u001b[0;32m     27\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32mc:\\Users\\marya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\marya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\combine\\_smote_enn.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditedNearestNeighbours\n",
      "File \u001b[1;32mc:\\Users\\marya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\over_sampling\\__init__.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_adasyn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ADASYN\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_random_over_sampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE, SMOTEN, SMOTENC, SVMSMOTE, BorderlineSMOTE, KMeansSMOTE\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADASYN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomOverSampler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\marya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE, SMOTEN, SMOTENC\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeansSMOTE\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVMSMOTE, BorderlineSMOTE\n",
      "File \u001b[1;32mc:\\Users\\marya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder, OrdinalEncoder\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     _get_column_indices,\n\u001b[0;32m     20\u001b[0m     _safe_indexing,\n\u001b[0;32m     21\u001b[0m     check_array,\n\u001b[0;32m     22\u001b[0m     check_random_state,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparsefuncs_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     csr_mean_variance_axis0,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _num_features\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_get_column_indices' from 'sklearn.utils' (c:\\Users\\marya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "%pip install scikeras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from typing import Tuple, List, Union, Optional\n",
    "from networkx import scale_free_graph\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from transformers import BertTokenizer\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard\n",
    "import datetime\n",
    "from scipy.spatial.distance import cosine\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "class ClassicalModels:\n",
    "    \"\"\"A class for processing and modeling code similarity data using classical ML approaches.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, target: str, max_length: int = 500):\n",
    "        \"\"\"\n",
    "        Initialize the ClassicalModels class.\n",
    "        \n",
    "        Args:\n",
    "            data: Input DataFrame containing code pairs\n",
    "            target: Target variable column name\n",
    "            max_length: Maximum sequence length for token padding\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.max_length = max_length\n",
    "        self.model: Optional[Pipeline] = None\n",
    "        self.batch_size = 32\n",
    "        self.epochs=50\n",
    "        \n",
    "        self.X_cosine=None\n",
    "        self.y=None\n",
    "        \n",
    "        \n",
    "        # Configure logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_java_string(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove Java code block markers from string.\n",
    "        \n",
    "        Args:\n",
    "            text: Input string containing Java code markers\n",
    "            \n",
    "        Returns:\n",
    "            String with Java markers removed\n",
    "        \"\"\"\n",
    "        return text.replace(\"```java\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "    def pad_sequence(self, tokens: List[int], max_length: Optional[int] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pad or truncate token sequence to fixed length.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of token IDs\n",
    "            max_length: Desired sequence length (defaults to self.max_length)\n",
    "            \n",
    "        Returns:\n",
    "            Padded numpy array of tokens\n",
    "        \"\"\"\n",
    "        max_length = max_length or self.max_length\n",
    "        tokens = np.array(tokens)\n",
    "        \n",
    "        if len(tokens) < max_length:\n",
    "            return np.pad(tokens, (0, max_length - len(tokens)), mode='constant')\n",
    "        return tokens[:max_length]\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List, List]:\n",
    "\n",
    "        try:\n",
    "            # Clean data\n",
    "            if 'Unnamed: 0' in df.columns:\n",
    "                df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "            \n",
    "            print(f\"Processing DataFrame with shape: {df.shape}\")\n",
    "            \n",
    "            # Clean Java strings\n",
    "            df['java_translation'] = df['java_translation'].apply(\n",
    "                lambda x: self.remove_java_string(str(x)) if isinstance(x, list) else self.remove_java_string(x)\n",
    "            )\n",
    "            \n",
    "            # Process token sequences\n",
    "            df['java_features'] = df['java_tokens'].apply(\n",
    "                lambda x: self.pad_sequence(eval(x) if isinstance(x, str) else x)\n",
    "            )\n",
    "            df['python_features'] = df['python_tokens'].apply(\n",
    "                lambda x: self.pad_sequence(eval(x) if isinstance(x, str) else x)\n",
    "            )\n",
    "            \n",
    "            # Prepare features and target\n",
    "            features = np.array(df['java_features'].tolist() + df['python_features'].tolist())\n",
    "            # target = df['Is_Equal'].tolist() * 2\n",
    "            target = np.array(df['Is_Equal'].tolist() * 2)\n",
    "            \n",
    "            #Scale Features\n",
    "            scaler=StandardScaler()\n",
    "            features=scaler.fit_transform(features)\n",
    "            \n",
    "\n",
    "            return features, target\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in preprocessing data: {str(e)}\")\n",
    "            raise\n",
    "    def engineer_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        # Parse embeddings if needed\n",
    "        df['python_tokens'] = df['python_tokens'].apply(\n",
    "            lambda x: np.array(ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "        )\n",
    "        df['java_tokens'] = df['java_tokens'].apply(\n",
    "            lambda x: np.array(ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "        )\n",
    "\n",
    "        # Pad sequences\n",
    "        df['python_tokens'] = df['python_tokens'].apply(self.pad_sequence)\n",
    "        df['java_tokens'] = df['java_tokens'].apply(self.pad_sequence)\n",
    "\n",
    "        # Feature Engineering\n",
    "        # 1. Absolute difference\n",
    "        abs_diff = df.apply(\n",
    "            lambda row: np.abs(row['python_tokens'] - row['java_tokens']), axis=1\n",
    "        )\n",
    "        X_abs_diff = np.stack(abs_diff)\n",
    "\n",
    "        # 2. Cosine similarity\n",
    "        cos_sim = df.apply(\n",
    "            lambda row: cosine_similarity([row['python_tokens']], [row['java_tokens']])[0][0],\n",
    "            axis=1\n",
    "        )\n",
    "        X_cos_sim = np.array(cos_sim).reshape(-1, 1)\n",
    "\n",
    "        # 3. Concatenated embeddings\n",
    "        concat_tokens = df.apply(\n",
    "            lambda row: np.concatenate((row['python_tokens'], row['java_tokens'])),\n",
    "            axis=1\n",
    "        )\n",
    "        X_concat = np.stack(concat_tokens)\n",
    "\n",
    "        # Combine features\n",
    "        X = np.hstack((X_abs_diff, X_cos_sim, X_concat))\n",
    "\n",
    "        return X\n",
    "    def preprocess_with_pca(self, df: pd.DataFrame, n_components: int = 60) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        try:\n",
    "            # Ensure tokens are properly formatted\n",
    "            df['java_tokens'] = df['java_tokens'].apply(\n",
    "                lambda x: eval(x) if isinstance(x, str) else x\n",
    "            )\n",
    "            df['python_tokens'] = df['python_tokens'].apply(\n",
    "                lambda x: eval(x) if isinstance(x, str) else x\n",
    "            )\n",
    "            \n",
    "            # Pad sequences\n",
    "            df['java_tokens'] = df['java_tokens'].apply(\n",
    "                lambda x: self.pad_sequence(x)\n",
    "            )\n",
    "            df['python_tokens'] = df['python_tokens'].apply(\n",
    "                lambda x: self.pad_sequence(x)\n",
    "            )\n",
    "            \n",
    "            # Convert to numpy arrays\n",
    "            X_java = np.array(df['java_tokens'].tolist())\n",
    "            X_python = np.array(df['python_tokens'].tolist())\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_java = scaler.fit_transform(X_java)\n",
    "            X_python = scaler.fit_transform(X_python)\n",
    "            \n",
    "            # Apply PCA\n",
    "            pca = PCA(n_components=n_components, random_state=42)\n",
    "            X_java_pca = pca.fit_transform(X_java)\n",
    "            X_python_pca = pca.fit_transform(X_python)\n",
    "            \n",
    "            # Calculate cosine similarities\n",
    "            cosine_similarities = []\n",
    "            for i in range(len(X_java_pca)):\n",
    "                similarity = 1 - cosine(X_java_pca[i], X_python_pca[i])\n",
    "                cosine_similarities.append(similarity)\n",
    "            \n",
    "            self.X_cosine = np.array(cosine_similarities).reshape(-1, 1)\n",
    "            self.y = df[self.target].values\n",
    "            \n",
    "            return self.X_cosine, self.y\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in PCA preprocessing: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def tokenize_data(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Tokenize Java and Python code using BERT tokenizers.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame containing code pairs\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize Java code\n",
    "            java_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            df['java_translation'] = df['java_translation'].apply(\n",
    "                lambda x: java_tokenizer.encode(x, add_special_tokens=True)\n",
    "            )\n",
    "            \n",
    "            # Tokenize Python code\n",
    "            python_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "            df['java_tokens'] = df['java_translation'].apply(\n",
    "                lambda x: python_tokenizer.encode(x, add_special_tokens=True)\n",
    "            )\n",
    "            df['python_tokens'] = df['original_code'].apply(\n",
    "                lambda x: python_tokenizer.encode(x, add_special_tokens=True)\n",
    "            )\n",
    "            \n",
    "            # Convert target to int\n",
    "            df['Is_Equal'] = df['Is_Equal'].astype(int)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in tokenizing data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def train_random_forest(self,features,target):\n",
    "        try:\n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                features, target, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Create and train pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', RandomForestClassifier(\n",
    "                    criterion='entropy',\n",
    "                    max_depth=8,\n",
    "                    max_features='sqrt',\n",
    "                    min_samples_leaf=8,\n",
    "                    min_samples_split=10\n",
    "                ))\n",
    "            ])\n",
    "            \n",
    "            pipeline.fit(X_train, y_train)\n",
    "            self.model = pipeline\n",
    "            \n",
    "            # Generate predictions\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            \n",
    "            # Log performance metrics\n",
    "            print(f\"RF  Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "            print(f\"RF Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "            print(f\"Rf Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "            print(f\"RF Train Score: {pipeline.score(X_train, y_train):.4f}\")\n",
    "            print(f\"RF Test Score: {pipeline.score(X_test, y_test):.4f}\")\n",
    "            \n",
    "            return pipeline\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in training random forest: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def neural_network(self,features,target) :\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "        # Define the model\n",
    "        model = Sequential([\n",
    "            Dense(128, input_shape=(self.max_length,), activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "                    # Compile the model\n",
    "        model.compile(\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            # Define callbacks\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.7,\n",
    "                patience=2,\n",
    "                min_lr=0.00001\n",
    "            )\n",
    "\n",
    "            # TensorBoard logging\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(\n",
    "                log_dir=log_dir,\n",
    "                histogram_freq=1\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "        print(\"Starting neural network training...\")\n",
    "        history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=self.epochs,\n",
    "                batch_size=self.batch_size,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[lr_scheduler, tensorboard_callback]\n",
    "            )\n",
    "\n",
    "            # Evaluate the model\n",
    "        train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "        print(f\"NN Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"NN Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "        # Generate predictions for test set\n",
    "        y_pred = model.predict(X_test) > 0.5\n",
    "        print(f\"NN Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "        print(f\"NN Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "        return model    \n",
    "\n",
    "    def GBoost(self,features, target):\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "        model= GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=10, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred=model.predict(X_test)\n",
    "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "        print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "        return model\n",
    "\n",
    "    def create_ensemble(self, features, target) -> VotingClassifier:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "        print(\"Creating model ensemble...\")\n",
    "\n",
    "        # Train individual models\n",
    "        rf_model = self.train_random_forest(X_train, X_test, y_train, y_test)\n",
    "        nn_model = KerasClassifier(build_fn=lambda: self.neural_network(X_train, X_test, y_train, y_test), epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
    "        gboost_model = self.GBoost(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        # Create a VotingClassifier\n",
    "        ensemble_model = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_model.named_steps['clf']),\n",
    "                ('gboost', gboost_model),\n",
    "                ('nn', nn_model)\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "\n",
    "        # Fit the ensemble model\n",
    "        ensemble_model.fit(X_train, y_train)\n",
    "        self.model = ensemble_model\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "        # Log performance metrics\n",
    "        \n",
    "        print(f\"Ensemble Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "        print(f\"Ensemble Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "        print(f\"Ensemble Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "        return ensemble_model\n",
    "\n",
    "    \n",
    "    def train_knn_with_smote(self, X: np.ndarray, y: np.ndarray) -> KNeighborsClassifier:\n",
    "\n",
    "        try:\n",
    "            # Apply SMOTE\n",
    "            smote = SMOTE(random_state=self.random_seed)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_resampled, y_resampled, test_size=0.2, random_state=self.random_seed\n",
    "            )\n",
    "            \n",
    "            # Define hyperparameter grid\n",
    "            param_grid = {\n",
    "                'n_neighbors': [3, 5, 7, 9, 11],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'metric': ['euclidean', 'manhattan']\n",
    "            }\n",
    "            \n",
    "            # Perform GridSearchCV\n",
    "            grid = GridSearchCV(\n",
    "                KNeighborsClassifier(),\n",
    "                param_grid,\n",
    "                cv=5,\n",
    "                scoring='accuracy',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            grid.fit(X_train, y_train)\n",
    "            \n",
    "            # Get best model\n",
    "            knn = grid.best_estimator_\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"Best Hyperparameters: {grid.best_params_}\")\n",
    "            \n",
    "            # Calculate and print metrics\n",
    "            y_train_pred = knn.predict(X_train)\n",
    "            y_test_pred = knn.predict(X_test)\n",
    "            \n",
    "            print(f\"KNN Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "            print(f\"KNN Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "            print(\"KNN Classification Report (Test):\")\n",
    "            print(classification_report(y_test, y_test_pred))\n",
    "            \n",
    "            # Store the model\n",
    "            self.X_train, self.X_test = X_train, X_test\n",
    "            self.y_train, self.y_test = y_train, y_test\n",
    "            \n",
    "            return knn\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in KNN training: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def train_svm(self, X: np.ndarray, y: np.ndarray, n_components: int = 400) -> SVC:\n",
    "\n",
    "        try:\n",
    "            # Reduce dimensionality with PCA\n",
    "            pca = PCA(n_components=n_components, random_state=self.random_seed)\n",
    "            X = pca.fit_transform(X)\n",
    "            \n",
    "            # Apply SMOTE\n",
    "            smote = SMOTE(random_state=self.random_seed)\n",
    "            X, y = smote.fit_resample(X, y)\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=self.random_seed, stratify=y\n",
    "            )\n",
    "            \n",
    "            # Store splits\n",
    "            self.X_train, self.X_test = X_train, X_test\n",
    "            self.y_train, self.y_test = y_train, y_test\n",
    "            \n",
    "            # Define parameter grid\n",
    "            param_grid = {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'kernel': ['linear', 'rbf', 'poly'],\n",
    "                'gamma': [0.01, 0.1],\n",
    "                'degree': [2, 3, 4]\n",
    "            }\n",
    "            \n",
    "            # Initialize SVM\n",
    "            svm = SVC(class_weight='balanced', random_state=self.random_seed)\n",
    "            \n",
    "            # Perform GridSearchCV\n",
    "            grid_search = GridSearchCV(\n",
    "                svm,\n",
    "                param_grid,\n",
    "                cv=5,\n",
    "                scoring='accuracy',\n",
    "                verbose=0,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            # Get best model\n",
    "            best_svm = grid_search.best_estimator_\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(best_svm, X_train, y_train, cv=5)\n",
    "            \n",
    "            # Predictions\n",
    "            train_pred = best_svm.predict(X_train)\n",
    "            test_pred = best_svm.predict(X_test)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "            print(f\"Cross-validation scores: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "            print(f\"SVM Train Accuracy: {accuracy_score(y_train, train_pred) * 100:.2f}%\")\n",
    "            print(f\"SVM Test Accuracy: {accuracy_score(y_test, test_pred) * 100:.2f}%\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_test, test_pred))\n",
    "            \n",
    "            return best_svm\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in SVM training: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "data=pd.read_csv('combined_df.csv')\n",
    "models=ClassicalModels(data, 'Is_Equal')\n",
    "models.tokenize_data(data)\n",
    "\n",
    "\n",
    "# features, target = models.preprocess_data(data)\n",
    "# models.train_random_forest(features,target)\n",
    "# models.neural_network(features,target)\n",
    "# models.GBoost(features,target)\n",
    "\n",
    "\n",
    "models.preprocess_with_pca(data)\n",
    "models.train_knn_with_smote(models.X_cosine, models.y)\n",
    "\n",
    "models.feature_engineering(data)\n",
    "models.train_svm(models.X, models.y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "3490e123979240ab9bad64d43564c7ed",
    "deepnote_cell_type": "code",
    "execution_context_id": "83d24c1b-8837-4559-b2e3-29a45d51476a",
    "execution_millis": 186,
    "execution_start": 1735863574451,
    "source_hash": "980a6872"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "\n",
    "# # pickle.dump(DTpipeline, open('DTpipeline.pkl', 'wb'))\n",
    "# pickle.dump(NNpipeline, open('NNpipeline.pkl', 'wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "e07c04f48f3842af9a8b370fd13ace6a",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
